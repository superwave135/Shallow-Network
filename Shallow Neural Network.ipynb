{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning - Practicum 05 - Shallow Network\n",
    "\n",
    "**Topics covered**: Implement the back propagation and prediction algorithms for a three layered feedforward neural network.\n",
    "\n",
    "**Deliverables**:\n",
    "- Complete the tasks as detailed in this document.\n",
    "- You are to implement the algorithms from scratch, numpy and scipy packages are allowed. \n",
    "\n",
    "**Objectives**:  \n",
    "This tutorial will help you get familiarized with feedforward neural network, its prediction and back propagation algorithms. The **Vectorization** method is encouraged to use.\n",
    "- How to construct a three layered fully connected neural network.\n",
    "- How to implement learning alogorithm - back propagation algorithm.\n",
    "- How to apply the learnt model for binary-classification problem.\n",
    "\n",
    "---\n",
    "Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import expit       # sigmoid\n",
    "import matplotlib.pyplot as plt       # plot the learning curve\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Structure of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feedforward neural network to be constructed consists of three layers: input, hidden and output layer. <br>\n",
    "\n",
    "- Input layer:  the number of neurons is determined by the number of features. <br>\n",
    "- Hidden layer: the number of neurons is specified by ther user. <br>\n",
    "- Output layer: one neuron for only binary-classifiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training data set - XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# input, truth table of XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Parameters of the model\n",
    "\n",
    "Implement function `ini_parameters` that takes in number of neurons at input layer and number of neurons at hidden layer, and return a parameter list. The list has 4 elements: <br>\n",
    "- weights connecting input layer and hidden layer, an array with shape (number_inputs, number_hidden) <br>\n",
    "- bias of hidden layer, an array with shape (1, number_hidden) <br>\n",
    "- weights connecting hidden layer and output layer, an array with shape (number_hidden, 1) <br>\n",
    "- bias of output layer, an array with shape (1,1)\n",
    "\n",
    "Note: use `np.random.default_rng(12345)` to initialize a random generator, we'll use those initial values generated by this generator to valide your code step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ini_parameters(number_inputs, number_hidden):\n",
    "    \n",
    "    para = []\n",
    "    \n",
    "    # random generator using a fixed seed\n",
    "    rng = np.random.default_rng(12345)\n",
    "    \n",
    "    # Your code goes here\n",
    "    w_input_hidden = rng.random((number_inputs, number_hidden))\n",
    "    para.append(w_input_hidden)\n",
    "\n",
    "    bias_hidden = rng.random((1, number_hidden))\n",
    "    para.append(bias_hidden)\n",
    "\n",
    "    w_hidden_output = rng.random((number_hidden, 1))\n",
    "    para.append(w_hidden_output)\n",
    "\n",
    "    # bias_output = rng.random((1, 1))\n",
    "    bias_output = rng.random((1, 1))\n",
    "    para.append(bias_output)\n",
    "\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.22733602, 0.31675834, 0.79736546],\n",
       "        [0.67625467, 0.39110955, 0.33281393]]),\n",
       " array([[0.59830875, 0.18673419, 0.67275604]]),\n",
       " array([[0.94180287],\n",
       "        [0.24824571],\n",
       "        [0.94888115]]),\n",
       " array([[0.66723745]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ini_parameters(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction\n",
    "\n",
    "Your `prediction` function takes in the samples in `data` and parameters `para` and returns the output for both hidden layer and output layer, i.e. `yhat`. It also gives the user an option of activation function for hidden layer, `sigmoid` or `tanh`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data, para, hidden_activation='tanh'):\n",
    "    \n",
    "    # Your code goes here   \n",
    "\n",
    "    z1 = np.dot(data, para[0]) + para[1]\n",
    "#     print(z1)\n",
    "\n",
    "    if hidden_activation == 'tanh':\n",
    "        a1 = np.tanh(z1)\n",
    "    elif hidden_activation == 'sigmoid': \n",
    "        a1 = 1 / (1 + np.exp(-z1)) \n",
    "        \n",
    "    z2 = np.tanh(z1)   \n",
    "#     a2 = expit(z2)\n",
    "    \n",
    "    hidden_output = z2\n",
    "    a2 = expit(a1.dot(para[2])+para[3])\n",
    "    yhat = a2\n",
    "    \n",
    "    return hidden_output, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate your function using the following code. The expected output is: 0.85.., 0.91.., 0.90.., 0.93.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85501841]\n",
      " [0.91106751]\n",
      " [0.90677819]\n",
      " [0.93059649]]\n",
      "[[0.53584502 0.18459359 0.58678988]\n",
      " [0.85502967 0.52109645 0.7639235 ]\n",
      " [0.67813052 0.46485941 0.89960063]\n",
      " [0.90549091 0.71365935 0.94710916]]\n"
     ]
    }
   ],
   "source": [
    "hidden_output, yhat = prediction(X, ini_parameters(2, 3))\n",
    "print(yhat)\n",
    "print(hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss function\n",
    "\n",
    "We want to moniter the learning pocess by looking at the Accuracy/Error procduced by parameters at each iteration. A loss function should be defined. Implement the function `likelihood_loss` to calculate the negative log likelihood loss on the given data with a particlular choice of parameters. <br>\n",
    "$$\n",
    "loss=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}logyhat^{(i)}+(1-y^{(i)})log(1-yhat^{(i)})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_loss(data, target, para):\n",
    "    \n",
    "    # Your code goes here\n",
    "#     loss = 0\n",
    "#     for i in range(len(yhat)):\n",
    "#         loss += (target[i]*np.log(yhat[i]) + (1-target[i])*(np.log(1-yhat[i])))\n",
    "#     loss = (-1/(len(yhat))) * loss\n",
    "    hidden_output, yhat = prediction(data, para, hidden_activation='tanh')\n",
    "    loss = - (1 / len(yhat)) * np.sum(np.multiply(target, np.log(yhat)) + np.multiply(1 - target, np.log(1 - yhat)))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate your function using the following code. The expected output is: 1.19.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1974905066304478"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_loss(X,Y,ini_parameters(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly review the steps in the learning process, each iteration: <br>\n",
    "- Take feedforward and get the ouput for each layers <br>\n",
    "- Take back propagation <br>\n",
    "    - Compute sensitivity for each neuron <br>\n",
    "    - Compute gradient for each weight <br>\n",
    "    - Update all parameters <br>\n",
    "- Compute the loss on train and validation set respectively, and save the optimal parameters basing on smallest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(train_data, train_target, validation_data, validation_target, hidden_neurons=3, alpha=0.5, hidden_activation='tanh', iteration=1000):\n",
    "    \n",
    "    # parameters:   train_data            featrue data for training the model \n",
    "    #               train_target          target for training\n",
    "    #               validation_data       feature data for validation\n",
    "    #               validation_target     target for validation\n",
    "    #               hidden_neurons        number of neurons at hidden layer\n",
    "    #               alpha                 learing rate as in gradient descent\n",
    "    #               hidden_activation     either \"tanh\" or \"sigmoid\"\n",
    "    #               iteration             maximal number of iteration\n",
    "    # return:       lossHistory           2 lists, one is train loss, another is valiation loss\n",
    "    #               the model             a list of updated parameters\n",
    "    \n",
    "    # Your code goes here\n",
    "    opt_para = ini_parameters(train_data.shape[1], hidden_neurons)\n",
    "    print(train_data.shape[1])\n",
    "    print(hidden_neurons)\n",
    "    aa = []\n",
    "    bb = []\n",
    "    LossHistory = [aa, bb]\n",
    "    for _ in range(iteration):\n",
    "#         opt_para = ini_parameters(train_data.shape[1], hidden_neurons)\n",
    "        output_from_hidden, yhat = prediction(train_data, opt_para, hidden_activation) # np.tanh(np.dot(X, weight_hidden) + bias_hidden)    \n",
    "        output_delta_list = -(train_target - yhat) * yhat * (1 - yhat)\n",
    "        delta_hidden = np.dot(output_delta_list, opt_para[2].T) * (1 - np.square(output_from_hidden))\n",
    "        opt_para[2] = opt_para[2] - alpha * (np.dot(output_from_hidden.T, output_delta_list) / 4)\n",
    "#         opt_para[3] = opt_para[3] - alpha * (np.sum(output_delta_list) / 4)\n",
    "        opt_para[3] = opt_para[3] - alpha * output_delta_list.mean()\n",
    "        opt_para[0] = opt_para[0] - alpha * (np.dot(train_data.T, delta_hidden) / 4)\n",
    "#         opt_para[1] = opt_para[1] - alpha * (np.sum(delta_hidden, axis = 0) / 4)\n",
    "        opt_para[1] = opt_para[1] - alpha * delta_hidden.mean(axis=0)\n",
    "    \n",
    "        aa.append(likelihood_loss(train_data, train_target, opt_para))\n",
    "        bb.append(likelihood_loss(validation_data, validation_target, opt_para))\n",
    "    \n",
    "    return LossHistory, opt_para;        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate your function using the following code. The expected output is: 0.12.., 0.88.., 0.88.., 0.15.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "[[0.12743758]\n",
      " [0.88900628]\n",
      " [0.88868061]\n",
      " [0.15958506]]\n"
     ]
    }
   ],
   "source": [
    "loss, parameters = back_propagation(X,Y,X,Y,3,0.5,\"tanh\")\n",
    "_, yhat = prediction(X,parameters)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves for the returned loss on validation data.\n",
    "\n",
    "#### Your code goes here\n",
    "plt.plot(loss[0])\n",
    "# plt.plot(loss[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "XOR truth table is an inseparable dataset. Logistic regreesion failed in building a model from it. Multi-layered neural network is proposed to bulid a classfier from inspeparable dataset. Nowadays it is becoming the foundation of deep learning structure. <br>\n",
    "If you have time, try explore your model by: <br>\n",
    "- changing the hyper parameters and observe the result and learning curve, e.g. number of neurons at hidden layer etc.\n",
    "- applying your model for other binary-classification problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
